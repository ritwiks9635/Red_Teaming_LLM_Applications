# Red_Teaming_LLM_Applications
**Red teaming** is a way of interactively testing AI models to protect against harmful behavior, including leaks of sensitive data and generated content that's toxic, biased, or factually inaccurate. Red teaming predates modern generative AI by many decades. 
